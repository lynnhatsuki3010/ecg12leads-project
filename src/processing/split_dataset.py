"""
processing/split_dataset.py
---------------------------
Táº¡o nhÃ£n multi-hot theo SNOMED CT vÃ  chia dá»¯ liá»‡u train/val/test.
LÆ°u káº¿t quáº£ ra thÆ° má»¥c splits/ Ä‘á»ƒ main.py cÃ³ thá»ƒ load trá»±c tiáº¿p.
"""

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


def create_multilabel_targets(df: pd.DataFrame, snomed_codes: list):
    """
    Táº¡o cá»™t multi-hot nhÃ£n tá»« danh sÃ¡ch SNOMED code.

    Args:
        df (pd.DataFrame): Báº£ng thÃ´ng tin bá»‡nh nhÃ¢n, cÃ³ cá»™t 'diagnosis_codes'.
        snomed_codes (list[str]): Danh sÃ¡ch mÃ£ bá»‡nh Ä‘Æ°á»£c giá»¯ láº¡i.

    Returns:
        tuple: (df má»›i cÃ³ cá»™t mÃ£ bá»‡nh, y: ma tráº­n nhÃ£n (num_samples, num_codes))
    """
    df = df.copy()
    for code in snomed_codes:
        df[code] = df["diagnosis_codes"].apply(
            lambda x: 1 if isinstance(x, str) and code in x.split(",") else 0
        )
    y = df[snomed_codes].values
    return df, y


def split_dataset(
    save_dir: str,
    df_resolved: pd.DataFrame,
    snomed_codes: list,
    test_size=0.3,
    val_ratio=2 / 3,
    random_state=42,
):
    """
    Chia dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ thÃ nh train/val/test vÃ  lÆ°u ra thÆ° má»¥c splits/.

    Args:
        save_dir (str): ThÆ° má»¥c chá»©a cÃ¡c file .npy Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ (tá»« bÆ°á»›c feature_extraction).
        df_resolved (pd.DataFrame): Báº£ng mapping giá»¯a file path vÃ  cháº©n Ä‘oÃ¡n.
        snomed_codes (list[str]): Danh sÃ¡ch SNOMED code cáº§n giá»¯ láº¡i.
        test_size (float): Tá»· lá»‡ test.
        val_ratio (float): Tá»· lá»‡ val trong pháº§n test+val.
        random_state (int): Seed cá»‘ Ä‘á»‹nh.
    """
    # === 1ï¸âƒ£ Táº¡o nhÃ£n multi-hot ===
    df_resolved, y = create_multilabel_targets(df_resolved, snomed_codes)

    # === 2ï¸âƒ£ Láº¥y danh sÃ¡ch file .npy ===
    all_files = [os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.endswith(".npy")]
    all_files.sort()  # Ä‘áº£m báº£o index nháº¥t quÃ¡n vá»›i df_resolved

    if len(all_files) == 0:
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file .npy nÃ o trong thÆ° má»¥c: {save_dir}")

    print(f"ğŸ”¹ Tá»•ng sá»‘ file tÃ­n hiá»‡u ECG: {len(all_files)}")

    # === 3ï¸âƒ£ Chia train/val/test ===
    train_files, temp_files = train_test_split(
        all_files, test_size=test_size, random_state=random_state
    )
    val_files, test_files = train_test_split(
        temp_files, test_size=val_ratio, random_state=random_state
    )

    print(f"âœ… Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}")

    # === 4ï¸âƒ£ HÃ m láº¥y index tá»« tÃªn file (record_123.npy -> 123) ===
    def get_index_from_filename(path):
        fname = os.path.basename(path)
        try:
            idx = int(fname.replace("record_", "").replace(".npy", ""))
            return idx
        except ValueError:
            raise ValueError(f"TÃªn file khÃ´ng há»£p lá»‡: {fname}")

    # === 5ï¸âƒ£ Ãnh xáº¡ nhÃ£n tÆ°Æ¡ng á»©ng ===
    y_train = np.array([y[get_index_from_filename(f)] for f in train_files])
    y_val = np.array([y[get_index_from_filename(f)] for f in val_files])
    y_test = np.array([y[get_index_from_filename(f)] for f in test_files])

    print("y_train shape:", y_train.shape)
    print("y_val shape:", y_val.shape)
    print("y_test shape:", y_test.shape)

    # === 6ï¸âƒ£ LÆ°u ra thÆ° má»¥c splits/ ===
    splits_dir = os.path.join(save_dir, "..", "splits")
    os.makedirs(splits_dir, exist_ok=True)

    np.save(os.path.join(splits_dir, "train_files.npy"), np.array(train_files))
    np.save(os.path.join(splits_dir, "val_files.npy"), np.array(val_files))
    np.save(os.path.join(splits_dir, "test_files.npy"), np.array(test_files))
    np.save(os.path.join(splits_dir, "y_train.npy"), y_train)
    np.save(os.path.join(splits_dir, "y_val.npy"), y_val)
    np.save(os.path.join(splits_dir, "y_test.npy"), y_test)

    print(f"ğŸ“ ÄÃ£ lÆ°u toÃ n bá»™ file chia táº­p vÃ o: {splits_dir}")

    return {
        "train": {"X": train_files, "y": y_train},
        "val": {"X": val_files, "y": y_val},
        "test": {"X": test_files, "y": y_test},
    }


# === 7ï¸âƒ£ HÃ m wrapper Ä‘á»ƒ gá»i tá»« main.py ===
def run_split_dataset():
    """
    HÃ m tiá»‡n Ã­ch Ä‘á»ƒ gá»i tá»« main.py â€” tá»± Ä‘á»™ng Ä‘á»c metadata vÃ  cháº¡y chia táº­p.
    """
    print("\n===== BÆ°á»›c 3: Chia dá»¯ liá»‡u train/val/test =====")

    # ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i
    base_dir = os.path.dirname(os.path.dirname(__file__))
    processed_dir = os.path.join(base_dir, "data", "processed", "ecg_npy")
    metadata_path = os.path.join(base_dir, "data", "processed", "patient_metadata_clean.xlsx")

    # Load metadata
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file metadata: {metadata_path}")

    df = pd.read_excel(metadata_path)

    # Danh sÃ¡ch mÃ£ SNOMED chÃ­nh (vÃ­ dá»¥ top 10)
    snomed_top10 = [
        "426177001","426783006","164890007","427084000",
        "427393009","164889003","429622005","39732003"
    ]

    # Cháº¡y chia táº­p
    split_dataset(save_dir=processed_dir, df_resolved=df, snomed_codes=snomed_top10)

    print("âœ… HoÃ n táº¥t chia dá»¯ liá»‡u!\n")


# === VÃ­ dá»¥ cháº¡y Ä‘á»™c láº­p ===
if __name__ == "__main__":
    run_split_dataset()
